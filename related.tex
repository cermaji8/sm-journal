\section{Related Work}
There is a number of algorithms designed for simultaneous-move games that can be classified into two categories: 
(1) exact algorithms, 
(2) approximative sampling algorithms.
\bbosansky{Add RW for Markov Games.}

\subsection{Exact Algorithms}
Exact algorithms are typically based on backward induction algorithm (e.g., see \cite{buro2003} \bbosansky{We do need some better reference here.}). 
The algorithm searches through the game tree in the depth-first manner and after computing the values of all the succeeding sub-games, it solves the normal-form game corresponding to this state (i.e., computes a NE of the matrix game in the current state of the game), and propagates the calculated value to the predecessor. The result of the backward-induction algorithm is a refinement of NE called \emph{subgame-perfect Nash equilibrium}.

There are two notable algorithms that improves the standard backward induction. 
Saffidine et al.~\cite{Saffidine12SMAB} termed simultaneous-move alpha-beta algorithm (SMAB). 
The main idea of the algorithm is to reduce the number of the recursive calls of the backward-induction algorithm by removing dominated actions in every stage game. The algorithm keeps bounds on the utility value for each successor in a game state. 
The lower and upper bounds represent the threshold values, for which neither of the actions of the player is dominated by any other action in the current matrix game. These bounds are calculated by linear programs in the state given existing exact values (or appropriate bounds) of the utility values of all the other successors of the state. If they form an empty interval (the lower bound is higher than the upper bound), a pruning occurs and the dominated action is not considered in this state any more. 

SMAB outperforms classical backward induction, however, the computation speed-up is not that dramatic. 
The reason is that computing dominated actions is a costly operations that does not prune many actions; hence, a lot of the game tree is still fully evaluated. The authors propose further enhancements in their work~\cite{Saffidine12SMAB}, however, these improvements are heuristic and do not significantly change the overall performance of the algorithm. Since other exact algorithms achieve computation speed-up in several orders of magnitude compared to backward induction (as we will show in experimental section of this chapter), we do not explicitly use SMAB in our experiments.

Second exact algorithm that is significantly faster compared to the classical backward induction was introduced in~\cite{Bosansky13Using}.
This algorithm is described in details in Section~\ref{sec:doab}, however, the main idea is to integrate two key components: (1) instead of evaluating all successors in each state of the game and solving a normal-form game, the algorithm exploits the iterative framework known in game theory as double-oracle algorithm~\cite{McMahan03Planning}; (2) the algorithm computes bounds on the utility values of the successors by serializing the sub-games and running a standard alpha-beta algorithm. 

Finally, since simultaneous-move games can be seen as standard extensive-form games with imperfect information, one can use state-of-the-art algorithms for solving generic extensive-form games with imperfect information that are based on sequence form linear programming~\cite{koller1996,bosansky2013-aamas}. However, these algorithms do not exploit the specific structure of simultaneous-move games causing, as we will show in the experiments, weak performance in practice.

\subsection{Approximative Sampling Algorithms}
Monte Carlo Tree Search (MCTS) is a simulation-based state space search technique often used in extensive-form games \cite{Coulom06,UCT}. 
Having first seen practical success in computer Go \cite{Gelly12}, MCTS has since been applied successfully to simultaneous-move games as well. 
Most of the successful applications use classical exploration/exploitation formula Upper Confidence Bounds (UCB)~\cite{UCB} as a selection strategy. These variants of MCTS are also known as UCT (UCB applied on trees). The first application of MCTS to simultaneous move games was in general game playing (GGP) \cite{Cadiaplayer} programs: the Cadiaplayer \cite{Cadiaplayer,Finnsson12} uses UCB selection strategy for each player in a single game tree. The success of MCTS algorithm was demonstrated by success of Cadiaplayer that was the top performing player of the GGP competition
between 2007 and 2009. 

Despite this success, Shafiei et al. in \cite{Shafiei09} provide a counter-example showing that this straightforward application of UCT does not
converge to NE even in the simplest simultaneous move games and that a player playing a NE can exploit this strategy. Another variant of UCT, which has been applied to the simultaneous move game Tron~\cite{Samothrakis10Tron}, builds the tree as if the players were moving sequentially giving one of the player unrealistic informational advantage. This approach also cannot converge to NE in general. For this reason, other variants of MCTS were considered for simultaneous move games. Teytaud and Flory describe a search algorithm for games with short-term imperfect information~\cite{Teytaud11Upper}, which are a generalization of simultaneous move games. Their algorithm uses a different selection strategy called Exp3~\cite{Auer2003Exp3} for the simultaneous moves and was shown to work well in the Internet card game Urban Rivals. 
A more thorough experimental investigation of different selection policies including UCB, UCB1-Tuned, UCB1-greedy, Exp3, and more is reported in the game of Tron \cite{Perick12Comparison}. Finally, works by Lanctot et al.~\cite{Lanctot13Goofspiel} and by Lisy et al.~\cite{lisy2013-nips} present variants of MCTS that provably converge to Nash equilibria in simultaneous move games.
