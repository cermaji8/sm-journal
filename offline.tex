This section focuses on problems of offline solving simultaneous-move games. 
The baseline algorithm for solving simultaneous-move games is backward induction~(Section~\ref{sec:algs:bi}).
Therefore, we start our description of the algorithms focusing on algorithms based on the backward induction, then we introduce algorithms based on no-regret learning and Monte Carlo sampling.
After formally describing the backward induction we present a modification that exploits quick calculation of upper and lower bounds in a simultaneous-move game~(Section~\ref{sec:algs:biab}).
Then, we further improve the algorithm by speeding up the calculation of NE in stage games, exploiting the iterative framework known as the double-oracle algorithm~(Seciton~\ref{sec:algs:doab}).
In Section~\ref{sec:algs:cfros} we present counterfactual regret minimization and its sampling variant outcome sampling. 
Finally, we describe Monte Carlo tree search for simultaneous move games in Section~\ref{sec:algs:smmcts}. 
%\bbosansky{CFR/MCTS introduction}

\subsection{Backward Induction}\label{sec:algs:bi}
Standard backward induction algorithm is based on the depth-first search that in each state of the game evaluates all successors, creates matrix game for the current state, solves the matrix game, and propagates back the value of the matrix game. The pseudocode of the algorithm is depicted in Algorithm~\ref{alg:backwardinduction}. When there is a chance node succeeding the current state $s$, the algorithm directly evaluates all successors of this chance node calculating an expected utility: the value of each subgame rooted in node $s'$ calculated by recursive call is weighted by the probability of the stochastic transition $\Delta_{\cT(s,r,c)}$~(line~\ref{alg:bi:recursive}).

\begin{algorithm2e}[t]
\small
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
\Input{$s$ -- current matrix game; $i$ -- searching player}
\If{$s \in \cZ$} {\Return $u_i(s)$} \label{alg:bi:stop1}
\For{$r \in \cA_1(s)$}{
	\For{$c \in \cA_2(s)$} {
	$A_{rc} \leftarrow \sum_{s' \in S \;:\; \Delta_{\cT(s,r,c)}(s') > 0} \Delta_{\cT(s,r,c)}(s')\cdot \textrm{BI}(s',i)$ \label{alg:bi:recursive}\;	
	}
}
$v_s \leftarrow$ solve matrix game $A$\;
\Return $v_s$ \label{alg:bi:stop2}
\caption{Backward Induction.}\label{alg:backwardinduction}
\end{algorithm2e}

Once the algorithm evaluates the value of each possible subgame following the current state $s$, then the matrix game $A$ is complete and the algorithm solves the matrix game $A$ using standard linear program (LP) for solving normal-form games:
\begin{eqnarray}
\max & v_s & \\
\textrm{s.t.} & \sum_{a_i \in \cA_{i}}A_{a_i,a_{-i}} \cdot \delta^{s}_i(a_i) \geq v_s & \forall a_{-i} \in \cA_{-i}\\
& \sum_{a_{i} \in \cA_{i}} \delta^{s}_{i}(a_{i}) = 1 \\
& \delta^{s}_{i}(a_{i}) \geq 0 & \forall a_{i} \in \cA_{i} 
\end{eqnarray}
Using the linear programming, the algorithm computes the value of the matrix game, which is propagated to the predecessor~(line~\ref{alg:bi:stop2}). 
If the algorithm evaluates a terminal state, it directly returns the utility value of the state~(line~\ref{alg:bi:stop1}).

\subsection{Backward Induction with Serialized Alpha-Beta Bounds}\label{sec:algs:biab}
Backward induction algorithm can be easily enhanced by using bounds on the value of sub-games. 
These bounds can be calculated very quickly by using a transformation of a simultaneous-move game into a perfect information extensive-form game.
Consider a matrix game representing a simultaneous choice of both players.
This matrix can be serialized by discarding the notion of information sets; hence, letting one player to play first following by the play of the second player. 
The crucial difference between a serialized and a simultaneous-move matrix game is that the second player to move has an advantage of knowing what action has been played in a serialized game.
Therefore, the value of a serialized game where player $i$ is second to move is an upper bound on the value 

\begin{figure}
\includegraphics[width=0.45\textwidth]{figures/serialization1-1.png}
\includegraphics[width=0.45\textwidth]{figures/serialization1-2.png}
\caption{Different serialization of a simultaneous move game.}\label{fig:serialization}
\end{figure}

An example of this serialization is depicted in Figure~\ref{fig:serialization} with the utility value in the leaves depicted only for a single player. 
If this player moves first (the left subfigure), then the value of this serialized game is the lower bound of the value of the game; if this player moves second (the right subfigure), then the value of this serialized game is the upper bound of the value of the game.
Since the serialized games are zero-sum perfect-information games in the extensive form, they can be solved very quickly by using classical AI algorithm such as alpha-beta or negascout.
If the values of both serialized games are equal, then this is equal also to the value of the game. This situation occurs in our example in Figure~\ref{fig:serialization}, where both serialized games have value equal to $3$.

Obtaining these bounds can speed-up the backward induction algorithm. 
Algorithm~\ref{alg:backwardinduction-ab} depicts the pseudocode.
When the backward induction starts evaluating successors of the current state, the algorithm calculates upper and lower bounds using alpha-beta algorithm on serialized variants of the subgame rooted in the successor $s'$ (lines~\ref{alg:biab:ab1}-\ref{alg:biab:ab2}). The serialized game is solved using standard alpha-beta algorithm, where the second player 
If the bounds are equal, the algorithm stores the value (line~\ref{alg:biab:saving}) instead of performing a recursive call~(line~\ref{alg:biab:recursive}).

\begin{algorithm2e}[t]
\small
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
\Input{$s$ -- current matrix game; $i$ -- searching player}
\If{$s \in \cZ$} {\Return $u_i(s)$} \label{alg:biab:stop1}
\For{$r \in \cA_1(s)$}{
	\For{$c \in \cA_2(s)$} {
		$A_{rc} \leftarrow 0$\;
		\For{$s' \in S \;:\; \Delta_{\cT(s,r,c)}(s') > 0$} {
		$v^i_{s'} \leftarrow \textrm{alpha-beta}(s',i)$\; \label{alg:biab:ab1}
		$v^{-i}_{s'} \leftarrow \textrm{alpha-beta}(s',-i)$\; \label{alg:biab:ab2}
		\If{$v^{-i}_{s'} < v^i_{s'}$} {
			$A_{rc} \leftarrow A_{rc} + \Delta_{\cT(s,r,c)}(s')\cdot \textrm{BI}\alpha\beta(s',i)$ \label{alg:biab:recursive}\;	
			}
		\Else{
			$A_{rc} \leftarrow A_{rc} + \Delta_{\cT(s,r,c)}(s')\cdot v^i_{s'}$ \label{alg:biab:saving}
			}
		}
	}
}
$v_s \leftarrow$ solve matrix game $A$\;
\Return $v_s$ \label{alg:biab:stop2}
\caption{Backward Induction with Serialized Bounds}\label{alg:backwardinduction-ab}
\end{algorithm2e}

\subsection{Backward Induction with Double Oracle and Serialized Bounds}\label{sec:algs:doab}
Solving a matrix game can be further improved by using iterative double-oracle algorithm~\cite{McMahan03Planning}. 
First of all, we describe the main principles of the double-oracle algorithm in normal-form games, following by describing the application of the algorithm in simultaneous-move game~\cite{Bosansky13Using}.

\subsubsection{Double-oracle Algorithm for Matrix Games}\label{sec:doab}
\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{figures/DO-scheme}
\caption{Schematic of the double-oracle algorithm for a normal-form game.}\label{fig:do-scheme}
\end{figure}

The main goal of the double-oracle algorithm is to find a solution of a matrix game without a need to construct the complete linear program that solves this game. 
The main idea is to create a restricted game by restricting the players to choose only from a limited set of actions to play.
Then the algorithm iteratively expands the restricted game by allowing the players to choose from new actions.
The new actions are allowed in iterations; each iteration a best response to an optimal strategy of the opponent in the current restricted game is allowed to be played in the restricted game.

Figure~\ref{fig:do-scheme} shows a visualization of the main structure of the algorithm, where the following three steps repeat until convergence:
\begin{enumerate}
\item create a restricted matrix game by limiting the set of actions that each player is allowed to play
\item compute a pair of Nash equilibrium strategies in this restricted game using linear programming
\item for each player, compute a pure best response strategy against the equilibrium strategy of the opponent; pure best response can be \emph{any} action from the original unrestricted game
\end{enumerate}
The best response strategies computed in step 3 are added to the restricted game, the game matrix is expanded by adding new rows and columns, and the algorithm follows with the next iteration. The algorithm terminates if neither of the players can improve the outcome of the game by adding a new strategy to the restricted game; hence, players play best response strategies to the strategy of the opponent. The algorithm maintains the values of expected utilities of the best-response strategies throughout the iterations of the algorithm. These values provide bounds on the value of the original game $v^*$

\subsubsection{Integrating Double-Oracle with Backward Induction}
\begin{algorithm2e}[t]
\small
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
\Input{$s$ -- current matrix game; $i$ -- searching player; $\alpha_s,\beta_s$ -- bounds for the game value rooted in state $s$}
\If{$s \in \cZ$} {\Return $u_i(s)$} \label{alg:doab:stop1}
\If{$v_s^{-i} = v_s^i$} {
	\Return $v_s^{i}$
}
initialize $\cA'_i$, $\cA'_{-i}$ with arbitrary actions from $\cA_i, \cA_{-i}$\; \label{alg:doab:init}
\Repeat{$\alpha_s = \beta_s$}{
	\For{$r \in \cA'_i$, $c \in \cA'_{-i}$}{\label{alg:doab:restr1}
		\If{$A'_{rc}$ is not initialized}{
			 $A'_{rc} \leftarrow 0$\;
			\For{$s' \in S \;:\; \Delta_{\cT(s,r,c)}(s') > 0$} {
			$v^i_{s'} \leftarrow \textrm{alpha-beta}(s',i)$\; 
			$v^{-i}_{s'} \leftarrow \textrm{alpha-beta}(s',-i)$\; 
			\If{$v^{-i}_{s'} < v^i_{s'}$} {
				$A'_{rc} \leftarrow A'_{rc} + \Delta_{\cT(s,r,c)}(s')\cdot \textrm{double-oracle}(s',i,v^{-i}_{s'},v^i_{s'})$ \label{alg:doab:recursive}\;	
				}
			\Else{
				$A'_{rc} \leftarrow A'_{rc} + \Delta_{\cT(s,r,c)}(s')\cdot v^i_{s'}$ \label{alg:doab:saving}
				}
			}
		}	
	}
	$\left\langle v_s, \delta' \right\rangle \leftarrow $ solve matrix game $A'$\; \label{alg:doab:NE}
	$\left\langle v^{BR}_i, a^{BR}_{i} \right\rangle \leftarrow $ best-response$(i, \delta'_{-i}, \alpha_s)$\;\label{alg:doab:br1}
	$\left\langle v^{BR}_{-i}, a^{BR}_{-i} \right\rangle \leftarrow $ best-response$(-i, \delta'_{i}, \beta_s)$\; \label{alg:doab:br2}
	$\alpha_s \leftarrow \max(\alpha_s, v^{BR}_i)$,
	$\beta_s \leftarrow \min(\beta_s, v^{BR}_{-i})$\; \label{alg:doab:bounds}
	$\cA'_i \leftarrow \cA'_i \cup \lbrace a^{BR}_i \rbrace$,
	$\cA'_{-i} \leftarrow \cA'_{-i} \cup \lbrace a^{BR}_{-i} \rbrace$ \label{alg:doab:expand}
}
\Return $v_s$ \label{alg:doab:stop2}
\caption{Double Oracle with Serialized Bounds}\label{alg:doab}
\end{algorithm2e}

Double-oracle algorithm for matrix games can be directly incorporated into the backward induction -- instead of immediately evaluating each of the successors of the current game state and solving the linear program, the algorithm can exploit the double-oracle algorithm. Pseudocode in Algorithm~\ref{alg:doab} depicts this integration. In each state of the game, the algorithm initializes the restricted game with an arbitrary action (line~\ref{alg:doab:init}). Afterwards, the algorithm needs to evaluate each of the successors of the restricted game, for which the current value is not known (lines~\ref{alg:doab:restr1}-\ref{alg:doab:saving}). This evaluation is same as for backward induction with alpha-beta algorithm. 

Once all values for the restricted game $A'$ are known, the algorithm solves the restricted game (line~\ref{alg:doab:NE}) and keeps the optimal strategies $\delta'$ of the restricted game. Next, the algorithm calculates best responses for each of the player~(lines~\ref{alg:doab:br1},\ref{alg:doab:br2}), and updates the lower and upper bounds (line~\ref{alg:doab:bounds}). Finally, the algorithm expands the restricted game with best response actions (line~\ref{alg:doab:expand}) until the lower and upper bound are equal. In this case, neither of the best responses improves the current solution from the restricted game; hence, the algorithm has found an equilibrium of the complete unrestricted matrix game corresponding to state $s$.


\begin{algorithm2e}[t]
\small
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
\Input{$s$ -- current matrix game; $i$ -- best-response player; $\lambda$ -- bound for the best-response value; $\delta'_{-i}$ -- strategy of the opponent }
$v^{BR}_i \leftarrow \lambda$ \;
$a_i^{BR} \leftarrow \textbf{null}$ \;
\For{$a_i \in \cA_i $\label{alg:br:start}} {%\setminus \cA'_i 
	$v_{a_i} \leftarrow 0$\;
	\For{$a_{-i} \in \cA'_{-i} \;:\; \delta'_{-i}(a_{-i}) > 0$} {\label{alg:br:opp}
		$\lambda_{a_i} \leftarrow \frac{v_i^{BR} - \sum_{a'_{-i} \in \cA'_{-i} \setminus \lbrace a_{-i} \rbrace} \delta'_{-i}(a'_{-i}) \cdot v^i_{\cT(s,a_i,a'_{-i})}}{\delta'_{-i}(a_{-i})}$\; \label{alg:br:bound}
		\If{	$\lambda_{a_i} > v^i_{\cT(s,a_i,a_{-i})}$}{
		continue from line~\ref{alg:br:start} with next $a_i$
		}
		\Else{
		\For{$s' \in S \;:\; \Delta_{\cT(s,a_i,a_{-i})}(s') > 0$} {
%			$v^i_{s'} \leftarrow \textrm{alpha-beta}(s',i)$\; \label{alg:br:ab1}
%			$v^{-i}_{s'} \leftarrow \textrm{alpha-beta}(s',-i)$\; \label{alg:br:ab2}
			\If{$v^{-i}_{s'} < v^i_{s'}$} {
				$v_{a_i,a_{-i}} \leftarrow v_{a_i,a_{-i}} + \Delta_{\cT(s,a_i,a_{-i})}(s')\cdot$ $\textrm{double-oracle}(s',i,v^{-i}_{s'},v^i_{s'})$\label{alg:br:recursive}\;	
				}
			\Else{
				$v_{a_i,a_{-i}} \leftarrow v_{a_i,a_{-i}} + \Delta_{\cT(s,a_i,a_{-i})}(s')\cdot v^i_{s'}$ \label{alg:br:saving}
				}
			}
			$v_{a_i} \leftarrow v_{a_i} + \delta'_{-i}(a_{-i})\cdot v_{a_i,a_{-i}}$\;		
		}
	}
	\If{$v_{a_i} > v_i^{BR}$}{\label{alg:br:max}
		$v_i^{BR} \leftarrow v_{a_i}$\;
		$a_i^{BR} \leftarrow a_i$\label{alg:br:save}
	} 
}
\Return $\langle v_i^{BR}, a_i^{BR} \rangle$ 
\caption{Best Response with Serialized Bounds}\label{alg:br}
\end{algorithm2e}

Next, we describe the algorithm for calculating the best responses. 
The pseudocode of the algorithm is depicted in Algorithm~\ref{alg:br}.
The goal of the algorithm is to find the best action from the original unrestricted game against current strategy of the opponent $\delta'$. 
Throughout the algorithm we use, as before, $v^i_{s'}$ to denote the upper bound of the value of the sub-game rooted in state $s'$ calculated using alpha-beta$(s',i)$. These values are calculated on demand, i.e., they are calculated once needed and cached until the game for state $s$ is not solved.
Moreover, once the algorithm calculates the exact value of a particular subgame, both upper and lower bounds are updated to be equal to actual value of the game. 

The algorithm iteratively tries all actions of player $i$ from the unrestricted game (line~\ref{alg:br:start}). 
Every action $a_i$ is evaluated against the actions of the opponent that are used in the optimal strategy from the restricted game (line~\ref{alg:br:opp}).
Before evaluating against an action of the opponent the algorithm determines, whether the current action of the searching player, $a_i$, can still be the best response action (line~\ref{alg:br:bound}). 
The value $\lambda_{a_i}$ represents the lowest possible expected utility this action must gain against the current action of the opponent $a_{-i}$. 
If this value is strictly higher than the upper bound of the successor (i.e., the value $v^i_{\cT(s,a_i,a_{-i})}$) than the algorithm knows that the action $a_i$ can never be the best response action, and the algorithm proceeds with the next action.
$\lambda_{a_i}$ is calculated by subtracting from the current best response value ($v_i^{BR}$) upper bound of the expected value against all other actions of the opponent ($v^i_{\cT(s,a_i,a'_{-i})}$). Recall, that these values are updated once the algorithm calculates exact values.

If the currently evaluated action $a_i$ can still be the best response, the value of the successor is determined (first by comparing the bounds). Once the expected outcome against all actions of the opponent is evaluated, the expected value of action $a_i$ is compared against the current best-response value (line~\ref{alg:br:max}) and saved, if the expected utility is higher~(line~\ref{alg:br:save}).

\subsection{Counterfactual Regret Minimization and Outcome Sampling} \label{sec:algs:cfros}

Counterfactual Regret (CFR) is a notion of regret at the information set level for extensive-form games with imperfect information~\cite{CFR}.
The CFR algorithm iteratively learns strategies in self-play, producing approximate equilibrium strategues over time. 

Define a {\it history} as a sequence of actions taken by all players (including chance) that starts from the beginning of the game. 
A history $h'$ is a prefix of another history $h$, denoted $h' \sqsubset h$, if $h$ contains $h'$ (as a prefix sequence of actions).  
The {\it counterfactual value} of reaching information set $I$ is the expected payoff given that player $i$ played to reach $I$, the opponents played
$\sigma_{-i}$ and both players played $\sigma$ after $I$ was reached:
\begin{equation}
\label{eq:cfv}
v_i(I,\sigma) = \sum_{(h,z) \in Z_I} \pi^{\sigma}_{-i}(h) \pi^{\sigma}_{i}(h,z) u_i(z), 
\end{equation}
where $Z_I = \{ (h,z)~|~z \in Z, h \in I, h \sqsubset z \}$.
Suppose, at time $t$, player $i$ plays with strategy $\sigma^t_i$.
Define $\sigma^t_{I \rightarrow a}$ as identical to $\sigma^t$ except at $I$ action $a$ is taken with probability $1$.
The counterfactual regret of not taking $a \in A(I)$ at time $t$ is $r^t(I,a) = v_i(I,\sigma^t_{I \rightarrow a}) - v_i(I,\sigma^t)$.
The CFR algorithm maintains the cumulative regret $R^T(I,a) = \sum_{t=1}^T r^t(I,a)$, for every action at every information set.
Then, the distribution at each information set for the next iteration $\sigma^{T+1}(I)$ is obtained individually using
regret-matching~\cite{Hart00}. The distribution is proportional to the positive portion of the individual actions' regret:
\begin{equation*}
\label{eq:rm}
\sigma^{T+1}(I,a) = \left\{
\begin{array}{ll}
R^{T,+}(I,a) / R^{T,+}_{sum}(I) & \mbox{if } R^{T,+}_{sum}(I) > 0 \\ 
1 / |A(I)|                   & \mbox{otherwise,}
\end{array} \right.
\end{equation*}
where $x^+ = \max(0,x)$ for any term $x$, and $R^{T,+}_{sum}(I) = \sum_{a' \in A(I)} R^{T,+}(I,a')$. 
Furthermore, the algorithm maintains for each information set the average   strategy profile
\begin{equation}
%\bar{\sigma}^T(I,a) = \frac{1}{T}\sum_{t=1}^T \sigma^t(I,a).
\bar{\sigma}^T(I,a) = \frac{\sum_{t=1}^T \pi^{\sigma^t}_i(I) \sigma^t(I,a)}{\sum_{t=1} \pi^{\sigma^t}_i(I)}, 
\end{equation}
where $\pi^{\sigma^t}_i(I) = \sum_{h \in I}\pi^{\sigma^t}_i(h)$.
The combination of the counterfactual regret minimizers in individual information sets also minimizes the overall 
average regret \cite{CFR}, and hence the average profile is a  $2\epsilon$-equilibrium, with $\epsilon \rightarrow 0$
as $T \rightarrow \infty$.

Monte Carlo Counterfactual Regret Minimization (MCCFR) applies CFR to sampled portions of the games~\cite{Lanctot09Sampling}.
In the {\it outcome sampling} (OS) variant, a single terminal history $z\in Z$ is sampled in each iteration.
The algorithm updates the regret in the information sets visited along $z$ using the
{\it sampled counterfactual value},
\begin{equation*}
\tilde{v}_i(I,\sigma) = \left\{
\begin{array}{ll}
\frac{1}{q(z)} \pi^{\sigma}_{-i}(z) \pi^{\sigma}_{i}(h,z) u_i(z) & \mbox{if } (h,z) \in Z_I\\
0  & \mbox{otherwise,}
\end{array} \right.
\label{eq:scv}
\end{equation*}
where $q(z)$ is the probability of sampling $z$.
As long as every $z \in Z$ has non-zero probability of being sampled, $\tilde{v}_i(I,\sigma)$ is an unbiased estimate of $v(I,\sigma)$
due to the importance sampling correction ($1/q(z)$). For this reason, applying CFR updates using these sampled counterfactual regrets
$\tilde{r}^t(I,a) = \tilde{v}_i(I,\sigma^t_{I \rightarrow a}) - \tilde{v}_i(I,\sigma^t)$
on the sampled information sets values also eventually converges to the approximate equilibrium of the game with high probability.
The required number of iterations for convergence is much larger, but each iteration is much faster.

Pseudocode of outcome sampling is presented in Section~\ref{sec:oos}.

% Will put pseudocode in the Online part..?
%\begin{algorithm2e}[t]
%\small
%\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
%\Input{game $g$}
%\Return 1
%\caption{Outcome Sampling MCCFR}\label{alg:os}
%\end{algorithm2e}



\subsection{Simultaneous-Move Monte Carlo Tree Search (SM-MCTS)} \label{sec:algs:smmcts}

Monte Carlo Tree Search (MCTS) is a simulation-based state space search algorithm often used
in game trees. The nodes in the tree represent game states. The main idea is to iteratively run
simulations to a terminal state, incrementally growing a tree rooted at the initial state of the game. In
its simplest form, the tree is initially empty and a single leaf is added each iteration. Each iteration
starts by visiting nodes in the tree, selecting which actions to take based on a selection function and
information maintained in the node. Consequently, the algorithm transitions to a successor state. When a
node is visited whose immediate children are not all in the tree, the node is expanded by adding a
new leaf to the tree. Then, \emph{a rollout policy} (e.g., random action selection) is applied from the new
leaf to a terminal state. The outcome of the simulation is then returned as a reward to the new leaf
and the information stored in the tree is updated.

\begin{algorithm2e}[t]
\small
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
\Input{$s$ -- current state of the game; $i$ -- current player}
\If{$s \in \cZ$} {
	\Return $u_i(s)$
}
\If{$s \in T \wedge \exists(a_i, a_{-i}) \in \cA(s)$~not previously visited} {
	choose one of the previously unselected $(a_i, a_{-i})$\;
	$s' \leftarrow \cT(s,a_i,a_{-i})$\;
	$T \leftarrow T \cup \lbrace s' \rbrace$\;
	$v_{s'} \leftarrow $ \emph{\underline{Rollout}}($s'$)\;
	$X_{s'} \leftarrow X_{s'} + v_{s'}$\;
	$n_{s'} \leftarrow n_{s'} + 1$\;
	\emph{\underline{Update}}$(s,a_i,a_{-i},v_{s'})$\;\label{alg:smmcts:up1}
	\Return \emph{\underline{RetVal}}$(v_{s'}, X_{s'}, n_{s'})$ 	
}
\Else{
	$(a_i, a_{-i}) \leftarrow$ \emph{\underline{Select}}$(s)$\;\label{alg:smmcts:select}
	$s' \leftarrow \cT(s,a_i,a_{-i})$\;
	$v_{s'} \leftarrow $ SM-MCTS($s',i$)\;\label{alg:smmcts:reccall}
	$X_{s} \leftarrow X_{s} + v_{s'}$\;
	$n_{s} \leftarrow n_{s} + 1$\;
	\emph{\underline{Update}}$(s,a_i,a_{-i},v_{s'})$\;\label{alg:smmcts:up2}
	\Return \emph{\underline{RetVal}}$(v_{s'}, X_{s}, n_{s})$ 	
}
\caption{Simultaneous Move Monte Carlo Tree Search}\label{alg:smmcts}
\end{algorithm2e}

Algorithm~\ref{alg:smmcts} describes a single iteration of SM-MCTS. $T$ represents the MCTS tree in which
each state is represented by one node. Every node $s$ maintains a cumulative reward sum over all
simulations through it, $X_s$, and a visit count $n_s$. Both are initially set to $0$. The critical parts of the
algorithm are the updates on lines~\ref{alg:smmcts:up1} and \ref{alg:smmcts:up2} and the selection on line \ref{alg:smmcts:select}. 
Different algorithms can be used as selection functions (e.g., UCB~\cite{UCB}, Exp3~\cite{Auer2003Exp3}, or regret matching~\cite{Hart00}) 
and we describe them in the following sections. 
Moreover, different selection functions need different statistics kept in each node; hence, function \emph{\underline{Update}} also differs. 
Finally, the algorithm returns a value based on \emph{\underline{RetVal}} function. 
In our algorithms we consider two different return value functions: the algorithm either directly returns value of the current 
state $v_{s'}$, or mean value $v_{s'}/n_{s'}$.

We now present several variants of SM-MCTS, though more variants can be found 
in~\cite{Perick12Comparison,Lanctot13Tron,Tak14smmcts}. Several examples are depicted in \cite{Tak14smmcts}. 

\subsubsection{Decoupled UCT} \label{sec:duct}

In Decoupled UCT (DUCT)~\cite{Cadiaplayer}, each player $i$ maintains separate reward sums $X^i_{s,a}$ and visit
counts $n^i_{s,a}$ for their own action set $a \in \cA_i(s)$.
When a joint action needs to be selected on line
\ref{alg:smmcts:select}, each player selects an action that maximizes the UCB value over their reward estimates independently:
\begin{equation}
a_i = \argmax_{a \in A_i(s)}{ \left\{ \bar{X}^i_{s,a} + C_i \sqrt{\frac{\ln n_s}{n_{s,a}}} \right\} }, 
  \mbox{ where } \bar{X}^i_{s,a} = \frac{X^i_{s,a}}{n_{s,a}}
\end{equation}
\noindent The update policy increases the rewards and visit counts for each player $i$: $X^i_{s,a_i} \leftarrow X^i_{s,a_i} + u_i$,
and $n_{s,a_i} \leftarrow n_{s,a_i} + 1$.

While references to children nodes in the MCTS tree are maintained in a matrix, each player {\it decouples} the values and estimates from the joint
actions space. In other words, for some state $s$, each player maintains their own tables of values. 
Many of the other selection policies also maintain values separately, and some use jointly
maintained values.

After the simulations, a move is chosen that maximizes $\bar{X}^i_{s,a_i}$ for the searching player $i$.
Alternatively, one can choose to play a mixed (\ie randomized) strategy by normalizing the visit counts.
We call the former DUCT(max) and the latter DUCT(mix).

\subsection{Exp3} \label{sec:exp3}

In Exp3~\cite{Exp3}, each player maintains an estimate of the sum of rewards, denoted $\hat{x}^i_{s,a}$, and visit counts $n^i_{s,a}$ for each of their actions.
The joint action selected on line \ref{alg:smmcts:select} is composed of an action independently selected for each player based on the probability
distribution. This probability of sampling action $a_i$ is

\begin{equation}
\label{eq:exp3select}
\sigma^t_i(s,a_i) = \frac{(1-\gamma) \exp(\eta w^i_{s,a_i})}{\sum_{a_j \in \cA_i(s)} \exp(\eta w^i_{s,a_j})} + \frac{\gamma}{|\cA_i(s)|}, \mbox{ where }
\end{equation}
\[ \eta = \frac{\gamma}{|\cA_i(s)|}, \mbox{ and } w^i_{s,a} = \hat{x}^i_{s,a} - \max_{a' \in \cA_i(s)} \hat{x}^i_{s,a'}. \]

\noindent Here, the reason to use $w^i_{s,a}$ is for numerical stability in the implementation.
The action selected by normalizing over the maximum value will be identical to the action chosen without normalizing.

The update after selecting actions $(a_1,a_2)$ and obtaining a simulation result $(u_1,u_2)$ updates the visits count
and adds to the corresponding reward sum estimates the reward divided by the probability that the action was played by the player using
$n_{s,a_i} \leftarrow n_{s,a_i} + 1$, and $\hat{x}^i_{s,a_i} \leftarrow \hat{x}^i_{s,a_i} + \frac{u_i}{\sigma^t_i(s,a_i)}$.
Dividing the value by the probability of selecting the corresponding action makes $\hat{x}^i_{s,a}$ estimate the sum of rewards over all 
iterations, not only the once where     $a_i$ was selected.

Since these values and strategies are maintained separately for each player,
Exp3 is decoupled in the same sense as DUCT, storing values separately. 

The mixed strategy used by player $i$ after the simulations are done is given by the frequencies of visit counts of the actions,
\[\sigma^{final}_i(s,a_i) = \frac{n_{s,a_i}}{\sum_{b_i\in A_i(s)} n_{s,b_i}}.\]

Previous work \cite{Teytaud11Upper} suggests first removing the samples caused by the exploration. 
This modification proved to be useful also in our experiments, so before      computing the resulting final mixed strategy, we set
\begin{equation}
n_{s,a_i} \leftarrow \max\left(0,n_{s,a_i} - \frac{\gamma}{|A_i(s)|}\sum_{b_i\in A_i(s)}n_{s,b_i}\right).
\end{equation}

\subsection{Regret Matching} \label{sec:rm}

This variant applies regret matching \cite{Hart00} to the current estimated matrix game at each stage.
Suppose iterations are numbered from $t \in \{ 1, 2, 3, \cdots \}$ and at each iteration and each decision node $s$
there is a mixed strategy $\sigma_i^t(s)$ used by each player $i$ for each node $s$ in the tree, initially set to
uniform random: $\sigma^0_i(s,a) = 1 / |\cA(s)|$.
Each player $i$ maintains a cumulative regret $r^i_s[a]$ for having played $\sigma_i^t(s)$ instead of $a \in \cA_i(s)$.
In addition, a table for the average strategy is maintained per player as
well $\bar{\sigma}^i_s[a]$. The values in both tables are initially set to 0.

On iteration $t$, the selection policy (line \ref{alg:smmcts:select} in Algorithm~\ref{alg:smmcts}) first builds
the player's current strategies from the cumulative regret. Define $x^+ = \max(x,0)$,
\begin{equation}
\label{eq:rm}
\sigma^t_i(s,a) = \frac{r^i_s[a]}{R^+_{sum}} \mbox{ if } R^+_{sum} > 0 
\mbox{ oth. } \frac{1}{|\cA_i(s)|}, \mbox{ where } R^+_{sum} = \sum_{a \in \cA_i(s)}{r^{i,+}_s[a]}. 
\end{equation}
The main idea is to adjust the strategy by assigning higher weight proportionally to actions based on the regret of
having not taken them over the long-term.
To ensure exploration, an $\gamma$-on-policy sampling procedure similar to Equation~\ref{eq:exp3select} is used
choosing action $a$ with probability $\gamma/|\cA(s)| + (1-\gamma) \sigma^t_i(s,a)$.

The updates on line \ref{alg:smmcts:up1} and \ref{alg:smmcts:up2} add regret accumulated at the iteration to
the regret tables $r^i_s$ and the average strategy $\bar{\sigma}^i_s[a]$. Suppose joint action $(a_1,a_2)$ is
sampled from the selection policy and utility $u_i$ is returned from the recursive call on line~\ref{alg:smmcts:reccall}.
Label the current child $(i,j)$ estimate $\bar{X}_{s,i,j}$ and the $reward(i,j) = \bar{X}_{s,i,j}$ if
$(i,j) \not= (a_1,a_2)$, or $u_i$ otherwise. The updates to the regret are:
\begin{eqnarray*}
\forall a_1' \in \cA_1(s), ~~  r^1_s[a_1'] \leftarrow r^1_s[a_1'] + ( reward(a_1', a_2) - u_1 ),\\
\forall a_2' \in \cA_2(s), ~~  r^2_s[a_2'] \leftarrow r^2_s[a_2'] + ( reward(a_1, a_2') - u_2 ),
\end{eqnarray*}
\noindent and average strategy updates for each player, $\bar{\sigma}^i_s[a] \leftarrow \bar{\sigma}^i_s[a] + \sigma^t_i(s,a).$

The regret values $r^i_s[a_i]$ are maintained separately by each player, as in DUCT. 
However, the updates and specifically the reward uses a value that is a function of the joint action space.

After the simulations, a move for the root $s$ is chosen by sampling over the strategy obtained by
normalizing the values in $\bar{\sigma}_s^i$.

